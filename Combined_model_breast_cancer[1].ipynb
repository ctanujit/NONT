{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrees = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.naive_bayes import BernoulliNB ## check\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTreeSplits(rf):\n",
    "    forest = rf.estimators_\n",
    "    featurelist=[]  #collection of arrays, one for each tree\n",
    "    threshlist=[]\n",
    "    trees = []\n",
    "\n",
    "    for i in range(len(forest)):\n",
    "        trees.append(forest[i].tree_)\n",
    "        featurelist.append(np.asarray(trees[i].feature))\n",
    "        threshlist.append(np.asarray(trees[i].threshold))\n",
    "\n",
    "    return (trees, featurelist, threshlist)\n",
    "\n",
    "\n",
    "def GetChildren(trees):\n",
    "    listcl = []\n",
    "    listcr = []\n",
    "    for i in range(len(trees)):\n",
    "        listcl.append(trees[i].children_left)\n",
    "        listcr.append(trees[i].children_right)\n",
    "    return(listcl, listcr)\n",
    "\n",
    "\n",
    "\n",
    "def GetActiveNodes(sample,featurelist, threshlist, trees):\n",
    "    #returns a list over all trees with all corresponding active nodes\n",
    "    #(featurelist, threshlist, trees) = GetTreeSplits(rf)\n",
    "\n",
    "    act_node_list=[]      #list, containing all reached nodes for each tree listed\n",
    "\n",
    "    for i in range(len(featurelist)):\n",
    "        actives = []\n",
    "        currentnode = 0\n",
    "        actives.append(0)\n",
    "\n",
    "        while featurelist[i][currentnode] != -2:    #not at end leaf yet\n",
    "\n",
    "            if sample[featurelist[i][currentnode]] >= threshlist[i][currentnode]:\n",
    "                currentnode = trees[i].children_right[currentnode]\n",
    "            else:\n",
    "                currentnode = trees[i].children_left[currentnode]\n",
    "\n",
    "            actives.append(currentnode)\n",
    "\n",
    "        act_node_list.append(actives)\n",
    "\n",
    "    return act_node_list\n",
    "\n",
    "def node_indicator_function(sample, rf):\n",
    "    #returns a vector of node activities over all tree nodes, over all trees.\n",
    "    #The output is\n",
    "    #zero for inactive nodes, and 1 for active nodes. In case of the queried\n",
    "    #node being a leaf node, the predicted class is returned as well.\n",
    "\n",
    "    (featurelist, threshlist) = GetTreeSplits(rf)\n",
    "    (act_node_list, active_feats_list, active_threshs_list) = GetActiveNodes(sample, rf)\n",
    "\n",
    "    label = 0\n",
    "    leafind = False\n",
    "    if act_node_list[treei].count(nodej) > 0:   #if active\n",
    "        indicator = 1.0\n",
    "        #print treei, nodej, len(act_node_list[treei])\n",
    "        if act_node_list[treei][len(act_node_list[treei])-1]==nodej: #if end leaf\n",
    "            label = rf.predict(sample)\n",
    "            leafind = True\n",
    "\n",
    "    else:\n",
    "        indicator = 0.0\n",
    "\n",
    "    return (indicator, label, leafind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from forest_functions import GetTreeSplits, GetChildren\n",
    "\n",
    "\n",
    "def InitFirstLayer(rf, strength01 = 1000.0):\n",
    "    \"\"\"\n",
    "    Given a fitted random regression forest model rf, this function returns\n",
    "    initialisation parameters for the first hidden layer of a neural net that\n",
    "    mimics the random forest's prediction behaviour.\n",
    "    - strength01 is a hyperparameter that determines how strongly the discrete\n",
    "    step function is approximated.\n",
    "    \"\"\"\n",
    "\n",
    "    # extract tree parameters\n",
    "    trees, featurelist, threshlist = GetTreeSplits(rf)\n",
    "    listcl, listcr = GetChildren(trees)\n",
    "\n",
    "    # layer sizes\n",
    "    HL1N = sum( [np.sum(tree.feature != -2) for tree in trees] )\n",
    "    n_inputs = rf.n_features_\n",
    "\n",
    "    # initialise first layer parameters\n",
    "    W1 = np.zeros( [n_inputs, HL1N], dtype = 'float32')\n",
    "    b1 = np.zeros([HL1N])\n",
    "    b1 = np.array(b1, dtype = 'float32')\n",
    "\n",
    "\n",
    "    currentnode = 0\t# index of HL1 neuron to be assigned an input weight\n",
    "    nodelist = []\t# list (over all trees) of list of network neurons\n",
    "                      # corresponding to tree splits\n",
    "\n",
    "    # for every tree\n",
    "    for i in range(len(trees)):\n",
    "        cl = listcl[i]\n",
    "        cr = listcr[i]\n",
    "\n",
    "        currentsplit = 0  # index for current node while moving through tree i\n",
    "        nodeCount = trees[i].node_count   # number of nodes in tree i\n",
    "        nlist = np.zeros([nodeCount, 1])  # for each treenode save a neuronindex\n",
    "\n",
    "        # go through all tree nodes in tree i\n",
    "        while currentsplit < nodeCount:\n",
    "            \"\"\"\n",
    "            Interpretation help:\n",
    "            #active with (+1) if Wx>b \t<==> active (+1) when split to the right\n",
    "            #active with (-1) if Wx<=b\t<==> active (-1) when split to the left.\n",
    "            \"\"\"\n",
    "\n",
    "            # What to do here: set weight and bias for current HL1 neuron\n",
    "            if featurelist[i][currentsplit] != -2:   #not leaf node\n",
    "                W1[featurelist[i][currentsplit], currentnode] = 1.0 * strength01\n",
    "                b1[currentnode] = -threshlist[i][currentsplit] * strength01\n",
    "\n",
    "                # store relationship from tree split index to HL1 neuron index\n",
    "                nlist[currentsplit] = currentnode\n",
    "                currentnode += 1\n",
    "\n",
    "            # Where to go next: identify next node in tree (depth first)\n",
    "            if cl[currentsplit] != -1: #( -1 means: empty child)\n",
    "                currentsplit = cl[currentsplit]\n",
    "            elif cr[currentsplit] != -1 :\n",
    "                currentsplit = cr[currentsplit]\n",
    "            else :\n",
    "                currentsplit += 1\n",
    "                #move to parent and next branch\n",
    "\n",
    "        nodelist.append(nlist) #saving list of neurons corresponding to tree i.\n",
    "\n",
    "    return W1, b1, nodelist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GetTreePaths(trees):\n",
    "    # List of lists containing the node indices for all paths through all trees\n",
    "    jointsplitindlists = []\n",
    "    # Litt of lists containing all path orders (left/right) through all trees\n",
    "    jointsplitorderlists = []\n",
    "\n",
    "    # lists of left and right children\n",
    "    listcl, listcr = GetChildren(trees)\n",
    "\n",
    "    for i in range(len(trees)):\n",
    "        paths, orders = [], []\n",
    "        cl = listcl[i].tolist()\n",
    "        cr = listcr[i]\n",
    "\n",
    "        leaf_nodes = np.where(cr == -1)[0].tolist()\n",
    "        cr = cr.tolist()\n",
    "\n",
    "        # for every leaf node get the path that led to it.\n",
    "        for leaf in leaf_nodes:\n",
    "            path, order = [], []\n",
    "            c = leaf\n",
    "            while c != 0:\n",
    "                #find mother node of c\n",
    "                if c in cl:\n",
    "                    mother = cl.index(c)\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    mother = cr.index(c)\n",
    "                    direction = +1\n",
    "                c = mother\n",
    "                path.append(c)\n",
    "                order.append(direction)\n",
    "\n",
    "            path.reverse()\n",
    "            order.reverse()\n",
    "            paths.append(path)\n",
    "            orders.append(order)\n",
    "\n",
    "        jointsplitindlists.append(paths)\n",
    "        jointsplitorderlists.append(orders)\n",
    "\n",
    "    return jointsplitindlists, jointsplitorderlists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#nodelist is from HL1\n",
    "def InitSecondLayer(rf, nodelist, strength12=0.1,  L2param=0.8):\n",
    "    \"\"\"\n",
    "    Given a fitted random regression forest model rf,\n",
    "    a nodelist from the previous layer initialisation and hyperparameters,\n",
    "    this function returns initialisation parameters for the second hidden layer\n",
    "    of a neural net that mimics the random forest's prediction behaviour.\n",
    "    \"\"\"\n",
    "\n",
    "    # extract tree paths\n",
    "    trees = [rf.estimators_[i].tree_ for i in range(rf.n_estimators) ]\n",
    "    jointsplitindlists, jointsplitorderlists = GetTreePaths(trees)\n",
    "\n",
    "    # layer sizes\n",
    "    HL1N = sum( [np.sum(tree.feature != -2) for tree in trees] )\n",
    "    HL2N = sum( [np.sum(tree.feature == -2) for tree in trees] )\n",
    "\n",
    "    # empty weight matrix and bias vector\n",
    "    W2 = np.zeros([HL1N,HL2N], dtype = 'float32')\n",
    "    b2 = np.zeros(HL2N, dtype = 'float32')\n",
    "\n",
    "    fneurons = []   #the feature neuron in HL1 that occurs in a given path\n",
    "    dneurons = []   #the path directions of the fneurons of a given path.\n",
    "    leaf_neurons = [] #for storing indices of HL2-neurons belonging to leafs\n",
    "    counter = 0\n",
    "\n",
    "    # for each tree i\n",
    "    for i in range(len(trees)):\n",
    "        #get relevant split info\n",
    "        indlist = jointsplitindlists[i]\n",
    "        orderlist = jointsplitorderlists[i]\n",
    "        neurons_used = []\n",
    "\n",
    "        # identify split neurons in HL1 and their desired direction\n",
    "        for k in range (len(indlist)):\n",
    "            fneurons.append( nodelist[i] [ indlist[k] ] )\n",
    "            dneurons.append( orderlist[k] )\n",
    "            neurons_used.append(counter)\n",
    "            counter +=1\n",
    "\n",
    "        #append HL2-neuron-indices used in tree i.\n",
    "        leaf_neurons.append(neurons_used)\n",
    "\n",
    "        # set input weights and biases for second layer\n",
    "        scndlayercount = 0\n",
    "        for k in range(len(fneurons)): # for every feature neuron used (in HL1)\n",
    "            inputns = fneurons[k]\n",
    "            dirs = dneurons[k]\n",
    "            for j in range(len(dirs)):\n",
    "                W2[int(inputns[j]), scndlayercount] = dirs[j] * strength12\n",
    "                b2[scndlayercount] = (-len(dirs)+0.5) * strength12\n",
    "            scndlayercount +=1\n",
    "\n",
    "    return W2, b2, leaf_neurons\n",
    "\n",
    "\n",
    "\n",
    "def InitThirdLayer(rf, leaf_neurons):\n",
    "    \"\"\"\n",
    "    Given a fitted random regression forest model rf,\n",
    "    and a list of leaf_neurons from the previous layer initialisation,\n",
    "    this function returns initialisation parameters for the third output layer\n",
    "    of a neural net that mimics the random forest's prediction behaviour.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    trees = [rf.estimators_[i].tree_ for i in range(rf.n_estimators) ]\n",
    "    ntrees = rf.n_estimators\n",
    "\n",
    "    # layer size\n",
    "    HL2N = sum( [np.sum(tree.feature == -2) for tree in trees] )\n",
    "\n",
    "    # empty weight vector. No bias, simply linear transformation.\n",
    "    W3 = np.zeros([HL2N, 1], dtype = 'float32')\n",
    "\n",
    "    # loop over trees\n",
    "    for i in range(len(trees)):\n",
    "        # identify the leaf node indices for tree i\n",
    "        leaf_ind = np.where(trees[i].feature ==-2)\n",
    "\n",
    "        # get the regression value for each of those leaves\n",
    "        leaf_values = [e[0][0] for e in trees[i].value[leaf_ind].tolist()]\n",
    "\n",
    "        # HL2 neurons corresponding to tree i\n",
    "        tree_neurons = leaf_neurons[i]\n",
    "\n",
    "        #compute  weights to output layer\n",
    "        for k in range(len(leaf_values)):\n",
    "            W3[tree_neurons[k]] = leaf_values[k] / float(ntrees) * 0.5\n",
    "\n",
    "    return W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_initialisation_parameters(rf, strength01=100.0, strength12=1.0):\n",
    "    \"\"\"\n",
    "    Given a pre-trained random forest model, this function returns as numpy arrays\n",
    "    the weights and biases for initialising a 2-layer feedforward neural network.\n",
    "    The strength01 and strength12 are hyperparameters that determine how strongly\n",
    "    the continuous neural network nonlinearity will approximate a discrete step function\n",
    "    \"\"\"\n",
    "\n",
    "    # get network parameters for first hidden layer\n",
    "    W1, b1, nodelist1 = InitFirstLayer(rf, strength01)\n",
    "\n",
    "    # get network parameters for second hidden layer\n",
    "    W2, b2, leaf_neurons = InitSecondLayer(rf, nodelist1, strength12)\n",
    "\n",
    "    # get network parameters for third hidden layer\n",
    "    W3 = InitThirdLayer(rf, leaf_neurons)\n",
    "\n",
    "    return W1, b1, W2, b2, W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_forward_pass(X, init_parameters, n_inputs, HL1N, HL2N, sigma=1.0,\n",
    "                        keep_sparse=True, n_layers=2):\n",
    "    \"\"\"\n",
    "    Defines a Multilayer Perceptron (MLP) model.\n",
    "    Inputs:\n",
    "        - X: tf placeholder [batchsize, n_inputs] for feature inputs\n",
    "        - init_parameters: tuple of numpy arrays (or None), holding values to initialise\n",
    "                the network weight matrices (and biases). If None, random values are\n",
    "                used for network initialisation.\n",
    "        - n_inputs: number of features\n",
    "        - HL1N: hidden layer 1 size\n",
    "        - Hl2N: hidden layer 2 size\n",
    "        - sigma: Variance of Gaussian distribution used when randomly initalising weights\n",
    "        - keep_sparse: whether to enforce network weight sparsity\n",
    "        - n_layers: number of hidden layers. Default case is 2, but can also run with 1 or 3,\n",
    "            but the number of neurons is then defined by HL1N, HL2N.\n",
    "            Note: The RF initialisation can only be used with n_layers = 2.\n",
    "    Outputs:\n",
    "        - predictions: tf tensor holding model predictions\n",
    "    \"\"\"\n",
    "\n",
    "    if n_layers == 2:   # default case.\n",
    "\n",
    "        # set the inital network parameter values, either to random, ...\n",
    "        if init_parameters is None:\n",
    "            # random initial parameters\n",
    "            W_01 = tf.Variable(tf.random_normal([n_inputs, HL1N], 0.0, sigma))\n",
    "            W_12 = tf.Variable(tf.random_normal([HL1N, HL2N], 0.0, sigma))\n",
    "            W_23 = tf.Variable(tf.random_normal([HL2N, 1], 0.0, sigma))\n",
    "            b_1 = tf.Variable(tf.random_normal([HL1N], 0.0, sigma))\n",
    "            b_2 = tf.Variable(tf.random_normal([HL2N], 0.0, sigma))\n",
    "            b_3 = tf.Variable(tf.random_normal([1], 0.0, sigma))\n",
    "\n",
    "        else:\n",
    "            # ... or to the specific values induced by the Random-Forest\n",
    "            W1, b1, W2, b2, W3 = init_parameters\n",
    "\n",
    "            W_01 = tf.Variable(W1)\n",
    "            W_12 = tf.Variable(W2)\n",
    "            W_23 = tf.Variable(W3)\n",
    "            b_1 = tf.Variable(b1)\n",
    "            b_2 = tf.Variable(b2)\n",
    "            b_3 = tf.Variable( np.sum(W3) )\n",
    "\n",
    "            if keep_sparse:\n",
    "                mask1 = tf.constant(np.float32(W1!=0.0))\n",
    "                mask2 = tf.constant(np.float32(W2!=0.0))\n",
    "                W_01 = tf.multiply(mask1, W_01)\n",
    "                W_12 = tf.multiply(mask2, W_12)\n",
    "\n",
    "        # defining the network with the given weights/biases\n",
    "        h = tf.nn.tanh(tf.matmul(X, W_01) + b_1)\n",
    "        h2 = tf.nn.tanh(tf.matmul(h, W_12) + b_2 )\n",
    "        prediction = tf.matmul(h2, W_23) + b_3\n",
    "\n",
    "\n",
    "    elif n_layers == 1:   # standard 1-layer MLP, initialised randomly\n",
    "        W_01 = tf.Variable(tf.random_normal([n_inputs, HL1N], 0.0, sigma))\n",
    "        W_12 = tf.Variable(tf.random_normal([HL1N, 1], 0.0, sigma))\n",
    "        b_1 = tf.Variable(tf.random_normal([HL1N], 0.0, sigma))\n",
    "        b_2 = tf.Variable(tf.random_normal([1], 0.0, sigma))\n",
    "        h = tf.nn.tanh(tf.matmul(X, W_01) + b_1)\n",
    "        prediction = tf.matmul(h, W_12) + b_2\n",
    "\n",
    "    elif n_layers == 3:   # standard 3-layer MLP, initialised randomly\n",
    "        W_01 = tf.Variable(tf.random_normal([n_inputs, HL1N], 0.0, sigma))\n",
    "        W_12 = tf.Variable(tf.random_normal([HL1N, HL2N], 0.0, sigma))\n",
    "        W_23 = tf.Variable(tf.random_normal([HL2N, HL2N], 0.0, sigma))\n",
    "        W_34 = tf.Variable(tf.random_normal([HL2N, 1], 0.0, sigma))\n",
    "        b_1 = tf.Variable(tf.random_normal([HL1N], 0.0, sigma))\n",
    "        b_2 = tf.Variable(tf.random_normal([HL2N], 0.0, sigma))\n",
    "        b_3 = tf.Variable(tf.random_normal([HL2N], 0.0, sigma))\n",
    "        b_4 = tf.Variable(tf.random_normal([1], 0.0, sigma))\n",
    "\n",
    "        h = tf.nn.tanh(tf.matmul(X, W_01) + b_1)\n",
    "        h2 = tf.nn.tanh(tf.matmul(h, W_12) + b_2 )\n",
    "        h3 = tf.nn.tanh(tf.matmul(h2, W_23) + b_3 )\n",
    "        prediction = tf.matmul(h3, W_34) + b_4\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_neural_net(XTrain, XTest, YTrain, YTest, init_parameters=None, HL1N=20, HL2N=10, n_layers=2,\n",
    "                   learning_rate=0.001, forest=None, keep_sparse=True,\n",
    "                   batchsize=32, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Trains / evaluates a Multilayer perceptron (MLP), potentially with a prespecified\n",
    "    weight matrix initialisation.\n",
    "    Inputs:\n",
    "    - data: tuple of input (X) - output (Y) data for train/dev/test set.\n",
    "        Output of dataloader.split_data\n",
    "    - init_parameters: output of initialiser.get_network_initialisation_parameters.\n",
    "        if init_parameters is set to None, random initial weights are picked.\n",
    "    - HL1N: number of neurons in first hidden layer\n",
    "    - HL2N: number of neurons in second hidden layer\n",
    "    - n_layers: number of hidden layers. Default 2, but can also be used with 1 and 3.\n",
    "    - verbose: how much to print\n",
    "    - learning_rate: used during training\n",
    "    - forest: a pre-trained random forest model. Not relevant when random initialisation is used.\n",
    "    - keep_sparse: whether to enforce weight matrix sparsity during training\n",
    "    - batchsize: used during training\n",
    "    - n_iterations: Number of training epochs\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"training MLP...\")\n",
    "#     XTrain, XValid, XTest, YTrain, YValid, YTest = data\n",
    "    n_samples, n_inputs = XTrain.shape\n",
    "    batchsize = min(batchsize, n_samples)\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "\n",
    "    # placeholders\n",
    "    X = tf.placeholder(\"float\", [None, n_inputs])\n",
    "    Y = tf.placeholder(\"float\", [None])\n",
    "\n",
    "    # forward pass\n",
    "    prediction = define_forward_pass(X, init_parameters, n_inputs, HL1N, HL2N, n_layers=n_layers)\n",
    "\n",
    "    # defining a RMSE objective function\n",
    "    loss = tf.reduce_mean(tf.pow(prediction - Y, 2) )\n",
    "    optimiser = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # define minibatch boundaries\n",
    "    batch_boundaries = list(zip(range(0, n_samples, batchsize), \\\n",
    "                range(batchsize, n_samples, batchsize)))\n",
    "    if n_samples % batchsize:\n",
    "        batch_boundaries += [(batch_boundaries[-1][1],n_samples)]\n",
    "    if len(batch_boundaries) == 0:\n",
    "        batch_boundaries += [(0,n_samples)]\n",
    "\n",
    "\n",
    "    RMSE_train, RMSE_valid, RMSE_test = [], [], []\n",
    "    pred_test_store = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(n_iterations):\n",
    "\n",
    "            for start, end in batch_boundaries:\n",
    "                #feed in training data minibatch-wise\n",
    "                sess.run(optimiser, feed_dict = {X: XTrain[start:end], \\\n",
    "                                                Y: YTrain[start:end]})\n",
    "\n",
    "            pred_train = sess.run(prediction, feed_dict={X: XTrain, Y: YTrain})\n",
    "            pred_test = sess.run(prediction, feed_dict={X: XTest, Y: YTest})\n",
    "            pred_test_store.append(pred_test)\n",
    "\n",
    "            diff_train = YTrain - pred_train\n",
    "            RMSE_train.append( np.sqrt(np.mean(np.square(diff_train ) ) ) )\n",
    "\n",
    "            diff_test = YTest - pred_test\n",
    "            RMSE_test.append( np.sqrt(np.mean(np.square(diff_test ) ) ) )\n",
    "\n",
    "    if forest is None:  # vanilla neural net\n",
    "        return RMSE_test[amin], pred_test_store[amin]\n",
    "    else:               # RF-initialised neural net\n",
    "        RF_predictions_test = forest.predict(XTest)\n",
    "        RF_score_test = np.sqrt( np.mean (np.square(RF_predictions_test-np.squeeze(YTest) ) )  )\n",
    "    return RF_score_test, RF_predictions_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"....\\wdbc.data\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave', 'points_worst_symmetry_worst', 'fractal_dimension_worst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave</th>\n",
       "      <th>points_worst_symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave  \\\n",
       "0            0.1622             0.6656           0.7119   0.2654   \n",
       "1            0.1238             0.1866           0.2416   0.1860   \n",
       "2            0.1444             0.4245           0.4504   0.2430   \n",
       "3            0.2098             0.8663           0.6869   0.2575   \n",
       "4            0.1374             0.2050           0.4000   0.1625   \n",
       "\n",
       "   points_worst_symmetry_worst  fractal_dimension_worst  \n",
       "0                       0.4601                  0.11890  \n",
       "1                       0.2750                  0.08902  \n",
       "2                       0.3613                  0.08758  \n",
       "3                       0.6638                  0.17300  \n",
       "4                       0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.DataFrame(preprocessing.scale(data.iloc[:,1:32]))\n",
    "datas.columns = list(data.iloc[:,1:32].columns)\n",
    "datas['diagnosis'] = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave</th>\n",
       "      <th>points_worst_symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "4          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  concave  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   0.2654   \n",
       "1      1956.0            0.1238             0.1866           0.2416   0.1860   \n",
       "2      1709.0            0.1444             0.4245           0.4504   0.2430   \n",
       "3       567.7            0.2098             0.8663           0.6869   0.2575   \n",
       "4      1575.0            0.1374             0.2050           0.4000   0.1625   \n",
       "\n",
       "   points_worst_symmetry_worst  fractal_dimension_worst  \n",
       "0                       0.4601                  0.11890  \n",
       "1                       0.2750                  0.08902  \n",
       "2                       0.3613                  0.08758  \n",
       "3                       0.6638                  0.17300  \n",
       "4                       0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set : 426 || Shape of test set : 143\n",
      "The dataset is very small so simple cross-validation approach should work here\n",
      "There are very few data points so 10-fold cross validation should give us a better estimate\n"
     ]
    }
   ],
   "source": [
    "predictors = data.columns[1:31]\n",
    "target = \"diagnosis\"\n",
    "\n",
    "X = data.loc[:,predictors]\n",
    "y = np.ravel(data.loc[:,[target]])\n",
    "\n",
    "# Split the dataset in train and test:\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "print ('Shape of training set : %i || Shape of test set : %i' % (x_train.shape[0],x_test.shape[0]) )\n",
    "print ('The dataset is very small so simple cross-validation approach should work here')\n",
    "print ('There are very few data points so 10-fold cross validation should give us a better estimate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network for tree 1 of 15\n",
      "The accuracy on test data is 0.96\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▌                                                                             | 1/15 [00:02<00:31,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.20483662259967567\n",
      "Training network for tree 2 of 15\n",
      "The accuracy on test data is 0.97\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████                                                                        | 2/15 [00:04<00:28,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.16724840200141816\n",
      "Training network for tree 3 of 15\n",
      "The accuracy on test data is 0.95\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 3/15 [00:06<00:27,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.2212488394343549\n",
      "Training network for tree 4 of 15\n",
      "The accuracy on test data is 0.97\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████▏                                                            | 4/15 [00:09<00:25,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.18698939800169145\n",
      "Training network for tree 5 of 15\n",
      "The accuracy on test data is 0.98\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████▋                                                       | 5/15 [00:11<00:22,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.14484136487558028\n",
      "Training network for tree 6 of 15\n",
      "The accuracy on test data is 0.98\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 6/15 [00:14<00:21,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.14484136487558028\n",
      "Training network for tree 7 of 15\n",
      "The accuracy on test data is 0.96\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|██████████████████████████████████████▋                                            | 7/15 [00:16<00:18,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.20483662259967567\n",
      "Training network for tree 8 of 15\n",
      "The accuracy on test data is 0.98\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|████████████████████████████████████████████▎                                      | 8/15 [00:18<00:16,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.14484136487558028\n",
      "Training network for tree 9 of 15\n",
      "The accuracy on test data is 0.96\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 9/15 [00:20<00:13,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.20483662259967567\n",
      "Training network for tree 10 of 15\n",
      "The accuracy on test data is 0.95\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████▋                           | 10/15 [00:23<00:11,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.2212488394343549\n",
      "Training network for tree 11 of 15\n",
      "The accuracy on test data is 0.94\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████████▏                     | 11/15 [00:25<00:08,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.23652495839563303\n",
      "Training network for tree 12 of 15\n",
      "The accuracy on test data is 0.95\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 12/15 [00:27<00:06,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.2212488394343549\n",
      "Training network for tree 13 of 15\n",
      "The accuracy on test data is 0.94\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|███████████████████████████████████████████████████████████████████████           | 13/15 [00:29<00:04,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.25087260300212727\n",
      "Training network for tree 14 of 15\n",
      "The accuracy on test data is 0.96\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████████████████████████████████████████▌     | 14/15 [00:31<00:02,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.20483662259967567\n",
      "Training network for tree 15 of 15\n",
      "The accuracy on test data is 0.95\n",
      "training MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for averaged prediction of individual networks: 0.2212488394343549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(ntrees)):\n",
    "    errors, preds = [], []\n",
    "    print (\"Training network for tree\", i+1, \"of\", ntrees )\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=15)\n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    predicted = rf.predict(x_test)\n",
    "    predictions = []\n",
    "    acc_test = metrics.accuracy_score(y_test, predicted)\n",
    "    print ('The accuracy on test data is %s' % (round(acc_test,2)))\n",
    "    init_parameters = get_network_initialisation_parameters(rf)\n",
    "    RMSE, pred = run_neural_net(x_train, x_test, y_train, y_test, init_parameters,\n",
    "                             forest=rf, keep_sparse=True)\n",
    "\n",
    "    errors += [RMSE]\n",
    "    predictions += [np.atleast_2d(np.ravel(pred))]\n",
    "\n",
    "    # average the (scalar) predictions across all trees\n",
    "    all_preds = np.concatenate(predictions, axis=0)\n",
    "    avg_pred = np.mean( all_preds, axis=0)\n",
    "\n",
    "    # compute RMSE\n",
    "    RMSE = np.sqrt( np.mean (np.square(avg_pred - np.squeeze(y_test)) )  )\n",
    "    print (\"score for averaged prediction of individual networks:\", RMSE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
